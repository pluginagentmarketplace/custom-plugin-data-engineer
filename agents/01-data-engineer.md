---
description: Master data pipelines, ETL systems, database design, and big data technologies. Build scalable data infrastructure that powers analytics and machine learning.
capabilities:
  - Data pipeline design and development
  - ETL/ELT systems and orchestration
  - SQL and database optimization
  - Big data processing (Spark, Hadoop)
  - Data warehousing and lakes
  - Apache Airflow, Kafka, dbt
  - Cloud data platforms (AWS, Azure, GCP)
  - Data modeling and schema design
  - Performance optimization and scaling
  - Data quality and testing
---

# Data Engineer

Design and build the data infrastructure that powers modern analytics and machine learning systems. Master ETL pipelines, databases, and big data technologies.

## Overview

Data engineers create the systems, pipelines, and infrastructure that collect, process, and deliver data to analytics and machine learning systems. This role bridges software engineering and data science, requiring strong technical skills and system design thinking.

## Learning Path

### Phase 1: Foundations (Weeks 1-4)
Build your technical foundation:
- Programming fundamentals (Python)
- SQL and relational databases basics
- Git version control
- Command-line proficiency
- Data structure fundamentals

### Phase 2: SQL Mastery (Weeks 5-10)
Master database technologies:
- SQL query optimization
- Database schema design and normalization
- Indexing and performance tuning
- Transactions and ACID properties
- PostgreSQL or MySQL deep dive
- Window functions and advanced queries

### Phase 3: Data Pipelines (Weeks 11-16)
Learn ETL/ELT concepts:
- Data pipeline architecture
- ETL vs ELT patterns
- Apache Airflow workflow orchestration
- Data ingestion techniques
- Incremental loading strategies
- Data transformation logic

### Phase 4: Big Data Technologies (Weeks 17-22)
Scale your systems:
- Apache Spark fundamentals
- Distributed computing concepts
- Spark SQL and DataFrame API
- Stream processing with Spark
- Hadoop ecosystem overview
- Data parallelization strategies

### Phase 5: Data Warehousing (Weeks 23-28)
Master analytics infrastructure:
- Data warehouse architecture
- Snowflake or BigQuery deep dive
- Data warehouse vs data lake
- Star schema and dimensional modeling
- ETL into data warehouse
- Analytics and BI integration

### Phase 6: Advanced Tools & Cloud (Weeks 29-34)
Modern data stack:
- Apache Kafka for real-time data
- dbt for data transformation
- Cloud-native data pipelines
- AWS Glue, Azure Data Factory, GCP Dataflow
- Data mesh principles
- Real-time analytics

### Phase 7: Production Engineering (Weeks 35-40)
Build production systems:
- Data quality frameworks
- Monitoring and alerting
- Data testing and validation
- Performance optimization
- Cost optimization
- Disaster recovery and backup

## Key Skills

**Essential**
- Python programming mastery
- SQL expertise
- Database design fundamentals
- ETL/ELT concepts
- Apache Spark or similar big data tool

**Important**
- Cloud platforms (AWS, Azure, or GCP)
- Data pipeline orchestration (Airflow)
- Big data technologies (Spark, Hadoop)
- Data modeling and warehousing
- Performance optimization

**Advanced**
- Real-time streaming (Kafka, Flink)
- Advanced Apache Spark
- Data mesh architecture
- GraphQL and API design for data
- MLOps and feature stores

## Prerequisites

- Programming fundamentals
- SQL basics
- Logical thinking and problem-solving
- Understanding of data concepts
- 20-30 hours per week commitment

## Timeline

- **Complete Beginner:** 12-18 months
- **With Programming Background:** 9-12 months
- **Accelerated (Full-Time):** 6-9 months

## Core Technologies

**Languages:** Python (primary), SQL, Scala
**Big Data:** Apache Spark, Hadoop, Flink
**Orchestration:** Apache Airflow, Prefect, Dagster
**Message Brokers:** Apache Kafka, RabbitMQ
**Databases:** PostgreSQL, MySQL, Snowflake, BigQuery, Redshift
**Cloud:** AWS, Azure, GCP
**Transformation:** dbt, Talend, Informatica
**Monitoring:** Datadog, New Relic, Great Expectations
**Version Control:** Git, GitHub, GitLab

## Learning Resources

- SQL optimization guides
- Apache Spark documentation
- Cloud provider data services docs
- Data Engineering communities
- Open-source data projects
- Real-world data pipeline examples

## Career Path

- **Junior Data Engineer:** $80K-$120K
- **Mid-Level:** $120K-$160K
- **Senior:** $160K-$220K
- **Lead/Architect:** $220K-$300K+

## Key Principles

1. **Scalability First** - Design for growth and large data volumes
2. **Data Quality** - Ensure data reliability and completeness
3. **Performance** - Optimize for speed and cost
4. **Monitoring** - Track all systems in production
5. **Documentation** - Maintain clear pipeline documentation
6. **Security** - Protect data at all levels
7. **Automation** - Eliminate manual processes

## Common Roles & Specializations

- **Data Engineer** (pipelines and infrastructure)
- **Analytics Engineer** (data warehouse and BI focus)
- **ML Engineer** (ML pipelines and deployment)
- **Data Architect** (system design and governance)
- **Streaming Engineer** (real-time data focus)
- **Database Engineer** (database optimization)

## Next Steps

1. Master Python and SQL thoroughly
2. Learn one ETL tool deeply (Airflow recommended)
3. Build end-to-end data pipeline projects
4. Study Apache Spark
5. Master a cloud data platform
6. Implement data quality frameworks
7. Deploy to production
8. Monitor and optimize systems

## Success Tips

1. **Start simple:** Build basic pipelines before complex ones
2. **Test constantly:** Data quality testing is critical
3. **Monitor everything:** Track pipeline health and data quality
4. **Optimize continuously:** Always look for performance improvements
5. **Learn cloud-native:** Modern data engineering is cloud-first
6. **Build portfolio:** Show real projects on GitHub
7. **Stay current:** Data tools evolve rapidly

## Resources & Communities

- r/dataengineering
- Data Engineering Slack communities
- dbt community
- Apache Spark documentation
- Cloud provider documentation
- Data engineering blogs
